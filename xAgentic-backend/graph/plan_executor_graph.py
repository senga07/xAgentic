"""
å¾ªç¯å¼è§„åˆ’æ‰§è¡Œå™¨ - æ¯ä¸ªèŠ‚ç‚¹å…ˆæ£€æŸ¥ä¸ç¡®å®šæ€§ï¼Œç¡®è®¤åæ‰§è¡Œ

æ‰§è¡Œæµç¨‹ï¼š
1. åˆ¶å®šè®¡åˆ’
2. å¾ªç¯æ‰§è¡Œï¼šæ£€æŸ¥èŠ‚ç‚¹ä¸ç¡®å®šæ€§ â†’ ç¡®è®¤ï¼ˆå¦‚éœ€è¦ï¼‰â†’ æ‰§è¡ŒèŠ‚ç‚¹
3. ç”Ÿæˆæœ€ç»ˆå›å¤
"""
from typing import List, TypedDict

from langchain_core.messages import AIMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, END
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt import create_react_agent
from langgraph.types import interrupt

from prompt.plan_executor_prompts import *
from services.service_manager import service_manager
from tools.code_tools import *
from tools.search_tools import *
from tools.time_tools import *
from utils import json_utils
from utils.custom_serializer import CustomSerializer
from utils.unified_logger import get_logger, log_error


class PlanExecutorState(TypedDict):
    """å¾ªç¯å¼è§„åˆ’æ‰§è¡ŒçŠ¶æ€"""
    # æ¶ˆæ¯å’Œé€šä¿¡
    messages: List[Any]
    streaming_chunks: List[Dict[str, Any]]

    # ä»»åŠ¡æ‰§è¡Œ
    task_analysis: str
    execution_plan: List[Dict[str, Any]]
    current_step: int
    step_results: List[Dict[str, Any]]
    final_response: str

    # çŠ¶æ€ç®¡ç†
    status: str
    error: str

    # ä¸Šä¸‹æ–‡ä¿¡æ¯
    thread_id: str

    # å…ƒæ•°æ®
    timing_info: Dict[str, Any]
    
    # Human-in-the-loop ç›¸å…³å­—æ®µ
    pending_confirmation: Dict[str, Any]  # å½“å‰ç­‰å¾…ç¡®è®¤çš„æ­¥éª¤ä¿¡æ¯
    user_feedback: str  # ç”¨æˆ·çš„åé¦ˆä¿¡æ¯
    is_replanning: bool  # æ˜¯å¦æ­£åœ¨é‡æ–°è§„åˆ’



class PlanExecutorGraph:
    """å¾ªç¯å¼è§„åˆ’æ‰§è¡Œå™¨ - æ¯ä¸ªèŠ‚ç‚¹å…ˆæ£€æŸ¥ä¸ç¡®å®šæ€§ï¼Œç¡®è®¤åæ‰§è¡Œ"""
    
    # ç±»çº§åˆ«çš„å›¾å®ä¾‹å’Œæ£€æŸ¥ç‚¹å­˜å‚¨ï¼Œç¡®ä¿æ‰€æœ‰å®ä¾‹å…±äº«ç›¸åŒçš„çŠ¶æ€
    _shared_checkpointer = None
    _shared_graph = None
    _initialized = False

    def __init__(self):
        self.thread_id = None
        self.logger = get_logger(__name__)

        # ä½¿ç”¨å…¨å±€æœåŠ¡ç®¡ç†å™¨ä¸­çš„é…ç½®å’ŒLLMå®ä¾‹
        self.config = service_manager.get_config()
        llms = service_manager.get_llms()
        self.plan_llm = llms.get('strategic_llm')
        self.worker_llm = llms.get('fast_llm')

        # è·å–å·¥å…·
        self.mcp_tools = service_manager.get_mcp_tools()
        self.local_tools = [
            web_search, execute_python_code, get_current_time,
            calculate_date_offset, get_time_info
        ]
        self.all_tools = self.local_tools + self.mcp_tools

        self.store = service_manager.store

        # ä½¿ç”¨å…±äº«çš„å›¾å®ä¾‹å’Œæ£€æŸ¥ç‚¹å­˜å‚¨
        if not PlanExecutorGraph._initialized:
            # åˆå§‹åŒ–LangGraphæ£€æŸ¥ç‚¹å­˜å‚¨
            custom_serializer = CustomSerializer()
            PlanExecutorGraph._shared_checkpointer = MemorySaver(serde=custom_serializer)
            
            # æ„å»ºå›¾
            PlanExecutorGraph._shared_graph = self._build_graph()
            PlanExecutorGraph._initialized = True
            self.logger.info("PlanExecutorGraph é¦–æ¬¡åˆå§‹åŒ–å®Œæˆ")
        
        # ä½¿ç”¨å…±äº«çš„å›¾å®ä¾‹
        self.graph = PlanExecutorGraph._shared_graph
        self.checkpointer = PlanExecutorGraph._shared_checkpointer
        self.logger.info("PlanExecutorGraph å®ä¾‹åˆ›å»ºå®Œæˆ")

    def _build_graph(self) -> CompiledStateGraph:
        """æ„å»ºå¾ªç¯å¼æ‰§è¡Œå›¾ - æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡Œå‰éƒ½æ£€æŸ¥ä¸ç¡®å®šæ€§"""
        workflow = StateGraph(PlanExecutorState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("analyze_and_plan", self._analyze_and_plan)
        workflow.add_node("check_node_uncertainty", self._check_node_uncertainty)
        workflow.add_node("execute_node", self._execute_node)
        workflow.add_node("generate_response", self._generate_response)

        # è®¾ç½®æµç¨‹
        workflow.set_entry_point("analyze_and_plan")
        workflow.add_edge("analyze_and_plan", "check_node_uncertainty")
        
        # ä¸ç¡®å®šæ€§æ£€æŸ¥åçš„æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "check_node_uncertainty",
            self._after_uncertainty_check,
            {
                "execute_node": "execute_node",  # æ‰§è¡Œå½“å‰èŠ‚ç‚¹
                "complete": "generate_response"  # æ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ
            }
        )

        # æ‰§è¡ŒèŠ‚ç‚¹åçš„æ¡ä»¶è¾¹
        workflow.add_conditional_edges(
            "execute_node",
            self._after_execute_node,
            {
                "next_node": "check_node_uncertainty",  # æ£€æŸ¥ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„ä¸ç¡®å®šæ€§
                "complete": "generate_response"  # æ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ
            }
        )

        workflow.add_edge("generate_response", END)

        return workflow.compile(checkpointer=PlanExecutorGraph._shared_checkpointer)

    # @trace_langsmith(name="analyze_and_plan", run_type="llm")
    def _analyze_and_plan(self, state: PlanExecutorState) -> PlanExecutorState:
        """ä»»åŠ¡åˆ†æå’Œè®¡åˆ’åˆ›å»ºèŠ‚ç‚¹"""
        try:
            self.logger.info("å¼€å§‹ä»»åŠ¡åˆ†æå’Œè®¡åˆ’åˆ›å»º")
            if "streaming_chunks" not in state:
                state["streaming_chunks"] = []

            start_time = time.time()
            user_input = self._get_user_input(state)
            if not user_input:
                raise Exception("æ— æ³•è·å–ç”¨æˆ·è¾“å…¥")
            messages = [HumanMessage(content=planning_prompt.format(user_task=user_input))]
            response = self.plan_llm.invoke(messages)

            plan_data = json_utils.json_match(response.content)
            if not plan_data or not plan_data.get("execution_plan"):
                raise Exception("è®¡åˆ’è§£æå¤±è´¥")

            state["task_analysis"] = plan_data.get("task_analysis", "")
            state["execution_plan"] = plan_data.get("execution_plan", [])
            state["current_step"] = 0
            state["step_results"] = []
            state["status"] = "planned"

            # æ·»åŠ æµå¼è¾“å‡º
            self._add_streaming_chunk(state, "planning", "ğŸ“‹ åˆ¶å®šæ‰§è¡Œè®¡åˆ’", {
                "task_analysis": state["task_analysis"],
                "execution_plan": state["execution_plan"]
            })

            total_duration = time.time() - start_time
            state["timing_info"] = {"plan_creation": {
                "duration": round(total_duration, 2),
                "timestamp": datetime.now().isoformat()
            }}

            self.logger.info(f"ä»»åŠ¡åˆ†æå’Œè®¡åˆ’åˆ›å»ºå®Œæˆï¼Œæ€»è€—æ—¶: {total_duration:.2f}ç§’")
            return state
        except Exception as e:
            log_error(self.logger, e)
            state["error"] = str(e)
            state["status"] = "failed"
            return state

    # @trace_langsmith(name="check_node_uncertainty", run_type="chain")
    def _check_node_uncertainty(self, state: PlanExecutorState) -> PlanExecutorState:
        """æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦éœ€è¦ç”¨æˆ·ç¡®è®¤"""

        self.logger.info("æ£€æŸ¥å½“å‰èŠ‚ç‚¹çš„ä¸ç¡®å®šæ€§")
        execution_plan = state.get("execution_plan")
        current_step = state.get("current_step")

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰èŠ‚ç‚¹éƒ½å·²å¤„ç†å®Œæˆ
        if current_step >= len(execution_plan):
            self.logger.info("æ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ")
            state["status"] = "all_nodes_completed"
            return state

        # è·å–å½“å‰è¦æ£€æŸ¥çš„èŠ‚ç‚¹
        current_node = execution_plan[current_step]

        # å¦‚æœèŠ‚ç‚¹éœ€è¦ç¡®è®¤ä¸”æ²¡æœ‰ç”¨æˆ·åé¦ˆï¼Œåˆ™ä¸­æ–­ç­‰å¾…ç¡®è®¤
        if current_node.get("requires_confirmation") and not state.get("user_feedback"):
            self.logger.info(f"èŠ‚ç‚¹ {current_step + 1} éœ€è¦ç”¨æˆ·ç¡®è®¤")

            # å‡†å¤‡ç¡®è®¤ä¿¡æ¯
            confirmation_info = {
                "type": "confirmation_required",
                "current_step": current_step + 1,
                "total_steps": len(execution_plan),
                "step_info": {
                    "step": current_node.get("step"),
                    "description": current_node.get("description"),
                    "uncertainty_reason": current_node.get("uncertainty_reason", ""),
                    "expected_result": current_node.get("expected_result")
                }
            }

            # ä¿å­˜å¾…ç¡®è®¤ä¿¡æ¯åˆ°çŠ¶æ€
            state["pending_confirmation"] = confirmation_info
            state["is_replanning"] = True

            # æ·»åŠ æµå¼è¾“å‡º
            self._add_streaming_chunk(
                state,
                "uncertainty_detected",
                f"âš ï¸ æ­¥éª¤ {current_step + 1} éœ€è¦ç¡®è®¤: {current_node.get('uncertainty_reason', '')}",
                confirmation_info
            )
            interrupt(confirmation_info)

        else:
            self.logger.info(f"èŠ‚ç‚¹ {current_step + 1} æ— éœ€ç¡®è®¤ï¼Œå¯ä»¥ç›´æ¥æ‰§è¡Œ")
            state["pending_confirmation"] = {}

        return state

    def _after_uncertainty_check(self, state: PlanExecutorState) -> str:
        """ä¸ç¡®å®šæ€§æ£€æŸ¥åçš„æ¡ä»¶åˆ¤æ–­"""
        if state.get("status") == "all_nodes_completed":
            return "complete"
        else:
            return "execute_node"


    # @trace_langsmith(name="execute_node", run_type="chain")
    def _execute_node(self, state: PlanExecutorState) -> PlanExecutorState:
        """æ‰§è¡Œå½“å‰èŠ‚ç‚¹"""

        start_time = time.time()
        execution_plan = state.get("execution_plan")
        current_step = state.get("current_step")

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰èŠ‚ç‚¹éƒ½å·²å¤„ç†å®Œæˆ
        if current_step >= len(execution_plan):
            self.logger.info("æ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ")
            state["status"] = "all_nodes_completed"
            return state

        # è·å–å½“å‰è¦æ‰§è¡Œçš„èŠ‚ç‚¹
        current_node = execution_plan[current_step]
        current_node["user_feedback"] = state.get("user_feedback", "")

        # æ·»åŠ æµå¼è¾“å‡º
        self._add_streaming_chunk(
            state,
            "executing_node",
            f"ğŸ”„ æ­£åœ¨æ‰§è¡ŒèŠ‚ç‚¹ {current_step + 1}/{len(execution_plan)}: {current_node.get('description', '')}",
            {"node_index": current_step, "total_nodes": len(execution_plan)}
        )

        # æ‰§è¡Œå½“å‰èŠ‚ç‚¹
        node_result = self._do_execute(current_node, current_step)

        # æ›´æ–°çŠ¶æ€
        state["step_results"].append(node_result)
        state["current_step"] += 1

        duration = time.time() - start_time
        if "timing_info" not in state:
            state["timing_info"] = {}
        state["timing_info"][f"node_{current_step + 1}"] = {
            "duration": round(duration, 2),
            "timestamp": datetime.now().isoformat()
        }

        # æ·»åŠ å®ŒæˆçŠ¶æ€
        self._add_streaming_chunk(
            state,
            "node_completed",
            f"âœ… èŠ‚ç‚¹ {current_step + 1} æ‰§è¡Œå®Œæˆ (è€—æ—¶: {duration:.2f}ç§’)",
            {
                "status": node_result.get("status", "completed"),
                "result": node_result,
                "step_number": current_step + 1,
                "execution_result": node_result.get("execution_result", ""),
                "timing": node_result.get("timing", {})
            }
        )

        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸
        if node_result.get('status') == 'failed':
            self.logger.error(f"èŠ‚ç‚¹ {current_step + 1} æ‰§è¡Œå¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ")
            state["status"] = "node_failed"
            state["error"] = f"èŠ‚ç‚¹ {current_step + 1} æ‰§è¡Œå¤±è´¥: {node_result.get('execution_result', '')}"

            self._add_streaming_chunk(
                state,
                "execution_failed",
                f"âŒ èŠ‚ç‚¹ {current_step + 1} æ‰§è¡Œå¤±è´¥ï¼Œåœæ­¢æ‰§è¡Œ",
                {"error": state["error"], "failed_node": current_step + 1}
            )
            return state

        state["status"] = "node_completed"
        state["is_replanning"] = False
        state["pending_confirmation"] = {}
        self.logger.info(f"èŠ‚ç‚¹ {current_step + 1} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
        return state


    def _after_execute_node(self, state: PlanExecutorState) -> str:
        """æ‰§è¡ŒèŠ‚ç‚¹åçš„æ¡ä»¶åˆ¤æ–­"""
        if state.get("status") == "all_nodes_completed":
            return "complete"
        elif state.get("status") == "node_failed":
            return "complete"  # å¤±è´¥ä¹Ÿç»“æŸæµç¨‹
        else:
            return "next_node"  # ç»§ç»­æ£€æŸ¥ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„ä¸ç¡®å®šæ€§


    def _do_execute(self, node: Dict[str, Any], node_index: int) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªèŠ‚ç‚¹"""
        start_time = time.time()

        try:
            # åˆ›å»ºReAct Agent
            agent = create_react_agent(
                model=self.worker_llm,
                tools=self.all_tools,
                checkpointer=self.checkpointer,
                store=self.store
            )

            # æ ¼å¼åŒ–å·¥å…·åˆ—è¡¨
            tools_list = []
            for tool in self.all_tools:
                if hasattr(tool, 'name') and hasattr(tool, 'description'):
                    tools_list.append(f"- {tool.name}: {tool.description}")
                else:
                    tools_list.append(f"- {tool.__name__ if hasattr(tool, '__name__') else str(tool)}")
            
            prompt = react_prompt.format(
                description=node.get("description"),
                expected_result=node.get("expected_result"),
                user_feedback=node.get("user_feedback"),
                tools="\n".join(tools_list)
            )

            # æ‰§è¡ŒèŠ‚ç‚¹
            try:
                result = agent.invoke(
                    {"messages": [{"role": "user", "content": prompt}]},
                    config={
                        "configurable": {"thread_id": self.thread_id},
                        "recursion_limit": 15,  # å¢åŠ é€’å½’é™åˆ¶
                        "max_execution_time": 60,  # å¢åŠ æ‰§è¡Œæ—¶é—´é™åˆ¶
                    }
                )
                self.logger.info(f"èŠ‚ç‚¹ {node_index + 1} æ‰§è¡Œç»“æœ: {type(result)} - {str(result)[:200]}...")
            except Exception as agent_error:
                self.logger.warning(f"ReAct Agent æ‰§è¡Œå¤±è´¥ï¼Œä½¿ç”¨ç›´æ¥ LLM è°ƒç”¨: {str(agent_error)}")
                # å›é€€åˆ°ç›´æ¥ LLM è°ƒç”¨
                from langchain_core.messages import HumanMessage
                messages = [HumanMessage(content=prompt)]
                result = self.worker_llm.invoke(messages)
                self.logger.info(f"ç›´æ¥ LLM è°ƒç”¨ç»“æœ: {type(result)} - {str(result)[:200]}...")

            # æå–ç»“æœ - æ”¹è¿›è¿”å›å€¼å¤„ç†
            execution_result = "èŠ‚ç‚¹æ‰§è¡Œå®Œæˆ"
            
            try:
                if result and isinstance(result, dict):
                    if "messages" in result:
                        messages = result["messages"]
                        if messages:
                            # æŸ¥æ‰¾æœ€åä¸€ä¸ªAIæ¶ˆæ¯
                            for msg in reversed(messages):
                                if hasattr(msg, 'content'):
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯AIæ¶ˆæ¯
                                    msg_type = str(type(msg)).lower()
                                    if 'ai' in msg_type or 'assistant' in msg_type:
                                        execution_result = msg.content
                                        break
                            else:
                                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°AIæ¶ˆæ¯ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªæ¶ˆæ¯
                                last_message = messages[-1]
                                if hasattr(last_message, 'content'):
                                    execution_result = last_message.content
                                else:
                                    execution_result = str(last_message)
                    elif "output" in result:
                        execution_result = str(result["output"])
                    else:
                        execution_result = str(result)
                elif result and hasattr(result, 'content'):
                    # å¦‚æœresultç›´æ¥æ˜¯æ¶ˆæ¯å¯¹è±¡ï¼ˆç›´æ¥LLMè°ƒç”¨ï¼‰
                    execution_result = result.content
                elif result:
                    # å…¶ä»–æƒ…å†µï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    execution_result = str(result)
                    
                # å¦‚æœç»“æœå¤ªçŸ­æˆ–åŒ…å«é”™è¯¯ä¿¡æ¯ï¼Œæä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯
                if len(execution_result) < 10 or "sorry" in execution_result.lower():
                    execution_result = f"ä»»åŠ¡æè¿°: {node.get('description', '')}\næ‰§è¡ŒçŠ¶æ€: å·²å®Œæˆ\nç»“æœ: {execution_result}"
                    
            except Exception as e:
                self.logger.error(f"æå–æ‰§è¡Œç»“æœæ—¶å‡ºé”™: {str(e)}")
                execution_result = f"æ‰§è¡Œå®Œæˆï¼Œä½†ç»“æœæå–æ—¶å‡ºç°é”™è¯¯: {str(e)}"

            duration = time.time() - start_time

            return {
                "step": node_index + 1,
                "execution_result": execution_result,
                "status": "completed",
                "timestamp": datetime.now().isoformat(),
                "timing": {
                    "start_time": datetime.fromtimestamp(start_time).isoformat(),
                    "end_time": datetime.now().isoformat(),
                    "duration": round(duration, 2)
                }
            }

        except Exception as e:
            end_time = time.time()
            duration = end_time - start_time

            return {
                "step": node_index + 1,
                "execution_result": f"èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(e)}",
                "status": "failed",
                "timestamp": datetime.now().isoformat(),
                "error_details": str(e),
                "timing": {
                    "start_time": datetime.fromtimestamp(start_time).isoformat(),
                    "end_time": datetime.now().isoformat(),
                    "duration": round(duration, 2)
                }
            }

    def _generate_response(self, state: PlanExecutorState) -> PlanExecutorState:
        """ç”Ÿæˆæœ€ç»ˆå›å¤"""
        try:
            start_time = time.time()
            self.logger.info("ç”Ÿæˆæœ€ç»ˆå›å¤")

            # ç”Ÿæˆå›å¤
            if state["step_results"]:
                final_result = state["step_results"][-1].get("execution_result", "")
            else:
                final_result = state["task_analysis"]

            state["final_response"] = final_result
            state["status"] = "completed"

            # æ·»åŠ åˆ°æ¶ˆæ¯å†å²
            if "messages" in state:
                state["messages"].append(AIMessage(content=final_result))
            else:
                state["messages"] = [AIMessage(content=final_result)]

            duration = time.time() - start_time
            if "timing_info" not in state:
                state["timing_info"] = {}
            state["timing_info"]["response_generation"] = {
                "duration": round(duration, 2),
                "timestamp": datetime.now().isoformat()
            }

            # è®¡ç®—æ€»æ—¶é—´
            total_duration = sum(
                info.get("duration", 0) for info in state["timing_info"].values()
            )

            state["streaming_chunks"].append({
                "step": "completed",
                "message": f"ğŸ‰ ä»»åŠ¡å®Œæˆï¼æ€»è€—æ—¶: {total_duration:.2f}ç§’",
                "data": {
                    "response": final_result,
                    "timing_info": state["timing_info"],
                    "total_nodes": len(state.get("execution_plan", [])),
                    "completed_nodes": len(state.get("step_results", [])),
                    "step_results": state.get("step_results", []),
                    "execution_plan": state.get("execution_plan", [])
                }
            })

            self.logger.info(f"å›å¤ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
            return state

        except Exception as e:
            log_error(self.logger, e, "å›å¤ç”Ÿæˆå¤±è´¥")
            state["error"] = str(e)
            state["status"] = "failed"
            return state

    def _get_user_input(self, state: PlanExecutorState):
        if "messages" in state and state["messages"]:
            for msg in reversed(state["messages"]):
                if isinstance(msg, HumanMessage):
                    return msg.content
        return None

    def _add_streaming_chunk(self, state: PlanExecutorState, step: str, message: str,
                             data: Dict[str, Any] = None) -> None:
        """æ·»åŠ æµå¼è¾“å‡ºå—çš„è¾…åŠ©æ–¹æ³•"""
        chunk = {
            "step": step,
            "message": message
        }
        if data:
            chunk["data"] = data
        state["streaming_chunks"].append(chunk)


    async def process_streaming_events(self, events, error_message_prefix="æ‰§è¡Œå¤±è´¥"):
        """å¤„ç†æµå¼äº‹ä»¶çš„å…¬å…±æ–¹æ³•"""
        try:
            async for event in events:
                self.logger.info(f"æ”¶åˆ°event: {event['event']} - {event.get('name', 'unknown')}")
                if event["event"] == "on_chain_stream":
                    chunk = event.get("data", {}).get("chunk", {})
                    if isinstance(chunk, dict) and "streaming_chunks" in chunk:
                        # è¾“å‡ºæµå¼å—
                        for streaming_chunk in chunk["streaming_chunks"]:
                            step_type = streaming_chunk.get("step", "")
                            yield {
                                "step": step_type,
                                "message": streaming_chunk.get("message", ""),
                                "data": streaming_chunk.get("data", {}),
                                "node": event.get("name", "unknown")
                            }
                    if isinstance(chunk, dict) and "__interrupt__" in chunk:
                        chunk_interrupt = chunk["__interrupt__"][0]
                        yield {
                            "step": "interrupt",
                            "message": "éœ€è¦ç”¨æˆ·ç¡®è®¤",
                            "data": chunk_interrupt.value,
                            "node": "check_node_uncertainty"
                        }
                        return
        except Exception as e:
            self.logger.error(f"æµå¼å¤„ç†å¤±è´¥: {str(e)}")
            yield {
                "step": "error",
                "message": f"{error_message_prefix}: {str(e)}",
                "data": {
                    "error": str(e),
                    "status": "failed"
                },
                "node": "error"
            }

    async def chat_with_planning_stream(self, thread_id: str, conversation_history: List = None, ):
        """æµå¼èŠå¤©æ¥å£ - æ”¯æŒè§„åˆ’æ‰§è¡Œæ¨¡å¼"""
        # åˆå§‹åŒ–çŠ¶æ€
        self.thread_id = thread_id
        initial_state = {
            "messages": conversation_history,
            "task_analysis": "",
            "execution_plan": [],
            "current_step": 0,
            "step_results": [],
            "final_response": "",
            "status": "initializing",
            "error": "",
            "streaming_chunks": [],
            "timing_info": {},
            "pending_confirmation": {},
            "user_feedback": {},
            "is_replanning": False
        }

        config = {"configurable": {"thread_id": self.thread_id}}
        events = self.graph.astream_events(initial_state, config=config, version="v1")
        
        async for chunk in self.process_streaming_events(events, error_message_prefix="æ‰§è¡Œå¤±è´¥"):
            yield chunk

    def get_current_state(self, config, current_state):

        if not current_state:
            try:
                # ä»æ£€æŸ¥ç‚¹è·å–æœ€æ–°çŠ¶æ€
                checkpoint_state = self.graph.get_state(config)
                if checkpoint_state and checkpoint_state.values:
                    return checkpoint_state.values
            except Exception as e:
                self.logger.warning(f"æ— æ³•ä»æ£€æŸ¥ç‚¹è·å–çŠ¶æ€: {e}")
        return None
